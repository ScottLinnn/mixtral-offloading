{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb","timestamp":1712545823877}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"43acf941fb594697bcf1c34d75726142":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27b1990c13f043fbac2299143f6fdaf1","IPY_MODEL_bf046d0b75dc432eb6a754e0bf8e7c34","IPY_MODEL_c03be762122346d49e95c8c7329ab8f1"],"layout":"IPY_MODEL_fada3e6045aa499f9cb91e8ea29fb644"}},"27b1990c13f043fbac2299143f6fdaf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8b0b757d1e6422982e70b38b0285494","placeholder":"​","style":"IPY_MODEL_0e500825442f45548bdca94a9d088a13","value":""}},"bf046d0b75dc432eb6a754e0bf8e7c34":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57300ebb34d14634b20a3b85c4c309da","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d8c5ee27bb84e19b1971f2f1dbfdf03","value":0}},"c03be762122346d49e95c8c7329ab8f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39a0ca92673d4c95a52c16dd6a1b7730","placeholder":"​","style":"IPY_MODEL_6a7449da8c344a0391d12731260b4390","value":" 0/0 [00:00&lt;?, ?it/s]"}},"fada3e6045aa499f9cb91e8ea29fb644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8b0b757d1e6422982e70b38b0285494":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e500825442f45548bdca94a9d088a13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57300ebb34d14634b20a3b85c4c309da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1d8c5ee27bb84e19b1971f2f1dbfdf03":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39a0ca92673d4c95a52c16dd6a1b7730":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a7449da8c344a0391d12731260b4390":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57633fca92b148ddb2b733719c549ded":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94f28927a91a42bf93198830341c213f","IPY_MODEL_502fb429965642c390494658a065ec0a","IPY_MODEL_e62fa49d36ed475aac137a94221c0131"],"layout":"IPY_MODEL_46770dd0e9c6461a846e8827083eff6a"}},"94f28927a91a42bf93198830341c213f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4245b60947e64696b5e3052890c8c300","placeholder":"​","style":"IPY_MODEL_b9ecc2ba872d4bf59d2ee18dce44e6f7","value":"config.json: 100%"}},"502fb429965642c390494658a065ec0a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb95f244a95748a6ad2001954d07019a","max":720,"min":0,"orientation":"horizontal","style":"IPY_MODEL_edb6c2ce70eb44a499c6bc442b407681","value":720}},"e62fa49d36ed475aac137a94221c0131":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19a255802baf4a64bda21d1b9a177116","placeholder":"​","style":"IPY_MODEL_8c9d28f257754274ac06b96b40fcd2ed","value":" 720/720 [00:00&lt;00:00, 21.7kB/s]"}},"46770dd0e9c6461a846e8827083eff6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4245b60947e64696b5e3052890c8c300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9ecc2ba872d4bf59d2ee18dce44e6f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb95f244a95748a6ad2001954d07019a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edb6c2ce70eb44a499c6bc442b407681":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19a255802baf4a64bda21d1b9a177116":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c9d28f257754274ac06b96b40fcd2ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c72d2f17b53d46808514bbb9b4754171":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30ef9e964db4485ca8abecf3c9d88970","IPY_MODEL_9debc1409c834580865f9d73266f3ca1","IPY_MODEL_7404e788816e4ab0b496a2d09f5b3955"],"layout":"IPY_MODEL_0f547a0f3bd84286b4036032c8e155dc"}},"30ef9e964db4485ca8abecf3c9d88970":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc04e174cfa745388ae7fa0bd347d889","placeholder":"​","style":"IPY_MODEL_5a776e746fed4ee596a643b6c6f74a9b","value":"Loading experts: 100%"}},"9debc1409c834580865f9d73266f3ca1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc76388352664d06ae3caf009c7c2c5a","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_309333ffebea477684ef49fe490a95d1","value":32}},"7404e788816e4ab0b496a2d09f5b3955":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c07c0fa69a2144a0b1a1ece492fce2a8","placeholder":"​","style":"IPY_MODEL_f9e9951b1d6d438a9fc4420f5a4f4339","value":" 32/32 [01:26&lt;00:00,  2.70s/it]"}},"0f547a0f3bd84286b4036032c8e155dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc04e174cfa745388ae7fa0bd347d889":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a776e746fed4ee596a643b6c6f74a9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc76388352664d06ae3caf009c7c2c5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"309333ffebea477684ef49fe490a95d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c07c0fa69a2144a0b1a1ece492fce2a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9e9951b1d6d438a9fc4420f5a4f4339":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Mixtral in Colab\n","\n","Welcome! In this notebook you can run [Mixtral8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) with decent generation speed **right in Google Colab or on a consumer-grade GPU**. This was made possible by quantizing the original model in mixed precision and implementing a MoE-specific offloading strategy.\n","\n","To learn more, read our [tech report](https://arxiv.org/abs/2312.17238) or check out the [repo](https://github.com/dvmazur/mixtral-offloading) on GitHub."],"metadata":{"id":"OW1moHJ1TdhO"}},{"cell_type":"markdown","source":["One will need approximately 16 GB of VRAM and 11 GB of RAM to run this notebook and generate somewhat long texts.\n","\n","\n","<details>\n","\n","<summary>How to balance between RAM and GPU VRAM usage</summary>\n","\n","You can balance between RAM and GPU VRAM usage by changing <code>offload_per_layer</code> variable in the <a href=\"#scrollTo=_mIpePTMFyRY&line=10&uniqifier=1\">Initialize model</a> section. Increasing <code>offload_per_layer</code> will decrease GPU VRAM usage, increase RAM usage and decrease generation speed. Decreasing <code>offload_per_layer</code> will have the opposite effect.\n","\n","Note that this notebook should run normally in Google Colab with <code>offload_per_layer = 4</code>, but may crush with other values. However, if you run this somewhere else, you're free to play with this variable.\n","</details>"],"metadata":{"id":"2-dvAX_hKZT4"}},{"cell_type":"markdown","source":["## Install and import libraries"],"metadata":{"id":"Y8MhvkC7TKEL"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/11868/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uqzt4OFzr7pv","executionInfo":{"status":"ok","timestamp":1712703530060,"user_tz":240,"elapsed":2464713,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}},"outputId":"3c4eafe5-a062-44d8-abba-f88e5228cfc3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/11868\n"]}]},{"cell_type":"code","source":["%cd mixtral-offloading\n","!pwd\n","!rm expert_cache_log.txt custom_layer_log.txt generated_text.txt generated_tokens.txt expert_cache.tsv\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BAGF4OjrJU1r","executionInfo":{"status":"ok","timestamp":1712703530423,"user_tz":240,"elapsed":368,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}},"outputId":"efc649e2-4f12-4b96-b3a2-c946d88b96d4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/11868/mixtral-offloading\n","/content/drive/MyDrive/11868/mixtral-offloading\n","rm: cannot remove 'expert_cache_log.txt': No such file or directory\n","rm: cannot remove 'generated_text.txt': No such file or directory\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"f7qY7ebqX7T7","executionInfo":{"status":"ok","timestamp":1712703725791,"user_tz":240,"elapsed":195370,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}}},"outputs":[],"source":["# fix numpy in colab\n","import numpy\n","from IPython.display import clear_output\n","\n","# fix triton in colab\n","!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia\n","\n","# !git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n","!pip install -q -r requirements.txt\n","!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo --quiet --local-dir Mixtral-8x7B-Instruct-v0.1-offloading-demo\n","\n","clear_output()"]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"IxvJ4HT4mrLh","executionInfo":{"status":"ok","timestamp":1712703725792,"user_tz":240,"elapsed":13,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}},"outputId":"24ddf4c5-14f9-4f9b-caac-42e7999d167a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/11868/mixtral-offloading\n"]}]},{"cell_type":"code","source":["import sys\n","\n","sys.path.append(\"mixtral-offloading\")\n","import torch\n","from torch.nn import functional as F\n","from hqq.core.quantize import BaseQuantizeConfig\n","from huggingface_hub import snapshot_download\n","from IPython.display import clear_output\n","from tqdm.auto import trange\n","from transformers import AutoConfig, AutoTokenizer\n","from transformers.utils import logging as hf_logging\n","\n","from src.build_model import OffloadConfig, QuantConfig, build_model"],"metadata":{"id":"GgpjnV7fV49W","colab":{"base_uri":"https://localhost:8080/","height":243,"referenced_widgets":["43acf941fb594697bcf1c34d75726142","27b1990c13f043fbac2299143f6fdaf1","bf046d0b75dc432eb6a754e0bf8e7c34","c03be762122346d49e95c8c7329ab8f1","fada3e6045aa499f9cb91e8ea29fb644","c8b0b757d1e6422982e70b38b0285494","0e500825442f45548bdca94a9d088a13","57300ebb34d14634b20a3b85c4c309da","1d8c5ee27bb84e19b1971f2f1dbfdf03","39a0ca92673d4c95a52c16dd6a1b7730","6a7449da8c344a0391d12731260b4390"]},"outputId":"33e1add4-0afe-4da1-d295-505cfa23a7ae","executionInfo":{"status":"ok","timestamp":1712703743008,"user_tz":240,"elapsed":17226,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["hqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43acf941fb594697bcf1c34d75726142"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n"]}]},{"cell_type":"markdown","source":["## Initialize model"],"metadata":{"id":"OkSYibHcTQsH"}},{"cell_type":"code","source":["model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n","quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n","state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n","\n","config = AutoConfig.from_pretrained(quantized_model_name)\n","\n","device = torch.device(\"cuda:0\")\n","\n","##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n","offload_per_layer = 4\n","# offload_per_layer = 5\n","###############################################################\n","\n","num_experts = config.num_local_experts\n","\n","offload_config = OffloadConfig(\n","    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n","    offload_size=config.num_hidden_layers * offload_per_layer,\n","    buffer_size=4,\n","    offload_per_layer=offload_per_layer,\n",")\n","\n","\n","attn_config = BaseQuantizeConfig(\n","    nbits=4,\n","    group_size=64,\n","    quant_zero=True,\n","    quant_scale=True,\n",")\n","attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n","\n","\n","ffn_config = BaseQuantizeConfig(\n","    nbits=2,\n","    group_size=16,\n","    quant_zero=True,\n","    quant_scale=True,\n",")\n","quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n","\n","\n","model = build_model(\n","    device=device,\n","    quant_config=quant_config,\n","    offload_config=offload_config,\n","    state_path=state_path,\n",")\n","\n"],"metadata":{"id":"_mIpePTMFyRY","executionInfo":{"status":"ok","timestamp":1712703877002,"user_tz":240,"elapsed":133995,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}},"colab":{"base_uri":"https://localhost:8080/","height":240,"referenced_widgets":["57633fca92b148ddb2b733719c549ded","94f28927a91a42bf93198830341c213f","502fb429965642c390494658a065ec0a","e62fa49d36ed475aac137a94221c0131","46770dd0e9c6461a846e8827083eff6a","4245b60947e64696b5e3052890c8c300","b9ecc2ba872d4bf59d2ee18dce44e6f7","bb95f244a95748a6ad2001954d07019a","edb6c2ce70eb44a499c6bc442b407681","19a255802baf4a64bda21d1b9a177116","8c9d28f257754274ac06b96b40fcd2ed","c72d2f17b53d46808514bbb9b4754171","30ef9e964db4485ca8abecf3c9d88970","9debc1409c834580865f9d73266f3ca1","7404e788816e4ab0b496a2d09f5b3955","0f547a0f3bd84286b4036032c8e155dc","dc04e174cfa745388ae7fa0bd347d889","5a776e746fed4ee596a643b6c6f74a9b","dc76388352664d06ae3caf009c7c2c5a","309333ffebea477684ef49fe490a95d1","c07c0fa69a2144a0b1a1ece492fce2a8","f9e9951b1d6d438a9fc4420f5a4f4339"]},"outputId":"5534a2d5-5d6a-43eb-d658-9d7265a2a30b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57633fca92b148ddb2b733719c549ded"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n","  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"]},{"output_type":"display_data","data":{"text/plain":["Loading experts:   0%|          | 0/32 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72d2f17b53d46808514bbb9b4754171"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Run the model"],"metadata":{"id":"Z4hBFYtPTUzT"}},{"cell_type":"code","source":["!ls"],"metadata":{"id":"tKmDaJXSnZdl","executionInfo":{"status":"ok","timestamp":1712703877174,"user_tz":240,"elapsed":181,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9da21127-8f0a-4568-f938-dd6e5bd73609"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["LICENSE  Mixtral-8x7B-Instruct-v0.1-offloading-demo  notebooks\tREADME.md  requirements.txt  src\n"]}]},{"cell_type":"code","source":["from transformers import TextStreamer\n","\n","with open(\n","    \"/content/drive/MyDrive/11868/mixtral-offloading/expert_cache.tsv\",\n","    \"w\",\n",") as f:\n","    f.write(\"UID0\\tUID1\\tEviction_Group\\tOffloaded\\tIndex\\n\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","past_key_values = None\n","sequence = None\n","\n","seq_len = 0\n","while True:\n","  print(\"User: \", end=\"\")\n","  user_input = input()\n","  print(\"\\n\")\n","\n","  user_entry = dict(role=\"user\", content=user_input)\n","  input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\", tokenize= True).to(device)\n","\n","  if past_key_values is None:\n","    attention_mask = torch.ones_like(input_ids)\n","  else:\n","    seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n","    attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n","\n","  # print(input_ids)\n","  print(f\"Decoded Tokens: {tokenizer.convert_ids_to_tokens(input_ids.squeeze(), skip_special_tokens=True)}\")\n","  print(\"\\n\")\n","\n","  print(\"Mixtral: \", end=\"\")\n","  result = model.generate(\n","    input_ids=input_ids,\n","    attention_mask=attention_mask,\n","    past_key_values=past_key_values,\n","    streamer=streamer,\n","    do_sample=True,\n","    temperature=0.9,\n","    top_p=0.9,\n","    max_new_tokens=512,\n","    pad_token_id=tokenizer.eos_token_id,\n","    return_dict_in_generate=True,\n","    output_hidden_states=True,\n","  )\n","  # print(result)\n","  print(\"\\n\")\n","\n","  sequence = result[\"sequences\"]\n","  input_len = len(input_ids.squeeze())\n","  output_token_ids = sequence.squeeze()[input_len:]\n","  output_tokens = tokenizer.convert_ids_to_tokens(output_token_ids.squeeze(), skip_special_tokens=True)\n","\n","  print(f\"input_ids shape: {input_ids.shape}\")\n","  print(f\"input_ids: {input_ids}\")\n","  print(f\"sequence shape: {sequence.shape}\")\n","  print(f\"sequence: {sequence}\")\n","  print(f\"output_token_ids: {output_token_ids}\")\n","  print(f\"output_tokens: {output_tokens}\")\n","  with open(\n","      \"/content/drive/MyDrive/11868/mixtral-offloading/generated_tokens.txt\", \"a\", encoding='utf-8'\n","  ) as f:\n","      for t in output_tokens:\n","        f.write(t)\n","        f.write(\"\\n\")\n","  #Introduce yourself, limit your response in 50 words.\n","  past_key_values = result[\"past_key_values\"]"],"metadata":{"id":"Zf4GkspecSm8","executionInfo":{"status":"error","timestamp":1712712909263,"user_tz":240,"elapsed":5940496,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"44483781-3014-49ae-f68b-1fe51f93e8f5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["User: Q: Which of the following statements are true concerning a triangular or recursive system?\\n\\ni) The parameters can be validly estimated using separate applications of OLS to\\n\\neach equation\\n\\n\\nii) The independent variables may be correlated with the error terms in other\\n\\nequations\\n\\n\\niii) An application of 2SLS would lead to unbiased but inefficient parameter estimates\\n\\n\\niv) The independent variables may be correlated with the error terms in the equations\\n\\nin which they appear as independent variables (a) (ii) and (iv) only (b) (i) and (iii) only (c) (i), (ii), and (iii) only (d) (i), (ii), (iii), and (iv)\\nA:\n","\n","\n","Decoded Tokens: ['▁[', 'INST', ']', '▁Q', ':', '▁Which', '▁of', '▁the', '▁following', '▁statements', '▁are', '▁true', '▁concerning', '▁a', '▁tri', 'angular', '▁or', '▁recurs', 'ive', '▁system', '?', '\\\\', 'n', '\\\\', 'ni', ')', '▁The', '▁parameters', '▁can', '▁be', '▁valid', 'ly', '▁estimated', '▁using', '▁separate', '▁applications', '▁of', '▁O', 'LS', '▁to', '\\\\', 'n', '\\\\', 'ne', 'ach', '▁equation', '\\\\', 'n', '\\\\', 'n', '\\\\', 'n', 'ii', ')', '▁The', '▁independent', '▁variables', '▁may', '▁be', '▁cor', 'related', '▁with', '▁the', '▁error', '▁terms', '▁in', '▁other', '\\\\', 'n', '\\\\', 'ne', 'qu', 'ations', '\\\\', 'n', '\\\\', 'n', '\\\\', 'n', 'iii', ')', '▁An', '▁application', '▁of', '▁', '2', 'SL', 'S', '▁would', '▁lead', '▁to', '▁un', 'bi', 'ased', '▁but', '▁in', 'efficient', '▁parameter', '▁estimates', '\\\\', 'n', '\\\\', 'n', '\\\\', 'n', 'iv', ')', '▁The', '▁independent', '▁variables', '▁may', '▁be', '▁cor', 'related', '▁with', '▁the', '▁error', '▁terms', '▁in', '▁the', '▁equations', '\\\\', 'n', '\\\\', 'n', 'in', '▁which', '▁they', '▁appear', '▁as', '▁independent', '▁variables', '▁(', 'a', ')', '▁(', 'ii', ')', '▁and', '▁(', 'iv', ')', '▁only', '▁(', 'b', ')', '▁(', 'i', ')', '▁and', '▁(', 'iii', ')', '▁only', '▁(', 'c', ')', '▁(', 'i', '),', '▁(', 'ii', '),', '▁and', '▁(', 'iii', ')', '▁only', '▁(', 'd', ')', '▁(', 'i', '),', '▁(', 'ii', '),', '▁(', 'iii', '),', '▁and', '▁(', 'iv', ')\\\\', 'n', 'A', ':', '▁[', '/', 'INST', ']']\n","\n","\n","Mixtral: The correct statement is (d): (i), (ii), and (iii) only.\n","\n","Here is the explanation for each statement:\n","\n","i) The parameters can be validly estimated using separate applications of OLS to each equation: This statement is not entirely true. While it is true that OLS can be used to estimate the parameters for each equation separately, this approach does not account for the potential correlation between the independent variables and the error terms in other equations. This correlation can lead to biased estimates, so it is generally not recommended to estimate the parameters of a simultaneous equations model in this way.\n","\n","ii) The independent variables may be correlated with the error terms in other equations: This statement is true. In a simultaneous equations model, the independent variables in one equation can be correlated with the error terms in other equations, leading to biased estimates of the coefficients. This correlation is known as the simultaneous equations bias.\n","\n","iii) An application of 2SLS would lead to unbiased but inefficient parameter estimates: This statement is not entirely true. While 2SLS can be used to eliminate the simultaneous equations bias, it does not necessarily provide efficient estimates of the coefficients. The efficiency of the estimates depends on the validity and strength of the instrumental variables used in the 2SLS procedure.\n","\n","iv) The independent variables may be correlated with the error terms in the equations in which they appear as independent variables: This statement is true. In a simultaneous equations model, the independent variables in one equation can be correlated with the error terms in the same equation, leading to biased estimates of the coefficients. This correlation is known as the classical measurement error or the classical errors-in-variables bias.\n","\n","Therefore, the correct answer is (d) (i), (ii), and (iii) only.\n","\n","\n","input_ids shape: torch.Size([1, 192])\n","input_ids: tensor([[    1,   733, 16289, 28793,  1186, 28747,  9595,   302,   272,  2296,\n","         12908,   460,  1132, 15942,   264,  2629,  8685,   442, 19681,   495,\n","          1587, 28804, 28756, 28711, 28756,  3023, 28731,   415,  5287,   541,\n","           347,  3716,   346, 11909,  1413,  7681,  8429,   302,   451,  6687,\n","           298, 28756, 28711, 28756,   485,   595,  8777, 28756, 28711, 28756,\n","         28711, 28756, 28711,  2773, 28731,   415,  7126,  8925,   993,   347,\n","          1275,  9646,   395,   272,  2118,  3471,   297,   799, 28756, 28711,\n","         28756,   485,   364,   697, 28756, 28711, 28756, 28711, 28756, 28711,\n","         19747, 28731,  1094,  4993,   302, 28705, 28750,  6255, 28735,   682,\n","          1736,   298,   521,  6309,  1293,   562,   297, 28627,  5621, 16438,\n","         28756, 28711, 28756, 28711, 28756, 28711,   449, 28731,   415,  7126,\n","          8925,   993,   347,  1275,  9646,   395,   272,  2118,  3471,   297,\n","           272, 12501, 28756, 28711, 28756, 28711,   262,   690,   590,  4305,\n","           390,  7126,  8925,   325, 28708, 28731,   325,  2773, 28731,   304,\n","           325,   449, 28731,   865,   325, 28726, 28731,   325, 28710, 28731,\n","           304,   325, 19747, 28731,   865,   325, 28717, 28731,   325, 28710,\n","           557,   325,  2773,   557,   304,   325, 19747, 28731,   865,   325,\n","         28715, 28731,   325, 28710,   557,   325,  2773,   557,   325, 19747,\n","           557,   304,   325,   449,  2681, 28711, 28741, 28747,   733, 28748,\n","         16289, 28793]], device='cuda:0')\n","sequence shape: torch.Size([1, 579])\n","sequence: tensor([[    1,   733, 16289, 28793,  1186, 28747,  9595,   302,   272,  2296,\n","         12908,   460,  1132, 15942,   264,  2629,  8685,   442, 19681,   495,\n","          1587, 28804, 28756, 28711, 28756,  3023, 28731,   415,  5287,   541,\n","           347,  3716,   346, 11909,  1413,  7681,  8429,   302,   451,  6687,\n","           298, 28756, 28711, 28756,   485,   595,  8777, 28756, 28711, 28756,\n","         28711, 28756, 28711,  2773, 28731,   415,  7126,  8925,   993,   347,\n","          1275,  9646,   395,   272,  2118,  3471,   297,   799, 28756, 28711,\n","         28756,   485,   364,   697, 28756, 28711, 28756, 28711, 28756, 28711,\n","         19747, 28731,  1094,  4993,   302, 28705, 28750,  6255, 28735,   682,\n","          1736,   298,   521,  6309,  1293,   562,   297, 28627,  5621, 16438,\n","         28756, 28711, 28756, 28711, 28756, 28711,   449, 28731,   415,  7126,\n","          8925,   993,   347,  1275,  9646,   395,   272,  2118,  3471,   297,\n","           272, 12501, 28756, 28711, 28756, 28711,   262,   690,   590,  4305,\n","           390,  7126,  8925,   325, 28708, 28731,   325,  2773, 28731,   304,\n","           325,   449, 28731,   865,   325, 28726, 28731,   325, 28710, 28731,\n","           304,   325, 19747, 28731,   865,   325, 28717, 28731,   325, 28710,\n","           557,   325,  2773,   557,   304,   325, 19747, 28731,   865,   325,\n","         28715, 28731,   325, 28710,   557,   325,  2773,   557,   325, 19747,\n","           557,   304,   325,   449,  2681, 28711, 28741, 28747,   733, 28748,\n","         16289, 28793,   415,  4714,  6251,   349,   325, 28715,  1329,   325,\n","         28710,   557,   325,  2773,   557,   304,   325, 19747, 28731,   865,\n","         28723,    13,    13, 15423,   349,   272, 13268,   354,  1430,  6251,\n","         28747,    13,    13, 28710, 28731,   415,  5287,   541,   347,  3716,\n","           346, 11909,  1413,  7681,  8429,   302,   451,  6687,   298,  1430,\n","          8777, 28747,   851,  6251,   349,   459,  8134,  1132, 28723,  4023,\n","           378,   349,  1132,   369,   451,  6687,   541,   347,  1307,   298,\n","         11679,   272,  5287,   354,  1430,  8777, 22264, 28725,   456,  4431,\n","          1235,   459,  2708,   354,   272,  4628, 17301,  1444,   272,  7126,\n","          8925,   304,   272,  2118,  3471,   297,   799, 12501, 28723,   851,\n","         17301,   541,  1736,   298,  4240,  1293, 16438, 28725,   579,   378,\n","           349,  6741,   459, 11572,   298, 11679,   272,  5287,   302,   264,\n","         15110,   607, 12501,  2229,   297,   456,  1069, 28723,    13,    13,\n","          2773, 28731,   415,  7126,  8925,   993,   347,  1275,  9646,   395,\n","           272,  2118,  3471,   297,   799, 12501, 28747,   851,  6251,   349,\n","          1132, 28723,   560,   264, 15110,   607, 12501,  2229, 28725,   272,\n","          7126,  8925,   297,   624,  8777,   541,   347,  1275,  9646,   395,\n","           272,  2118,  3471,   297,   799, 12501, 28725,  5374,   298,  4240,\n","          1293, 16438,   302,   272, 19955, 28723,   851, 17301,   349,  2651,\n","           390,   272, 15110,   607, 12501, 16117, 28723,    13,    13, 19747,\n","         28731,  1094,  4993,   302, 28705, 28750,  6255, 28735,   682,  1736,\n","           298,   521,  6309,  1293,   562,   297, 28627,  5621, 16438, 28747,\n","           851,  6251,   349,   459,  8134,  1132, 28723,  4023, 28705, 28750,\n","          6255, 28735,   541,   347,  1307,   298, 20545,   272, 15110,   607,\n","         12501, 16117, 28725,   378,  1235,   459, 11867,  3084,  9096, 16438,\n","           302,   272, 19955, 28723,   415, 12832,   302,   272, 16438,  9618,\n","           356,   272,  3716,   472,   304,  6342,   302,   272, 28166,  8925,\n","          1307,   297,   272, 28705, 28750,  6255, 28735, 10722, 28723,    13,\n","            13,   449, 28731,   415,  7126,  8925,   993,   347,  1275,  9646,\n","           395,   272,  2118,  3471,   297,   272, 12501,   297,   690,   590,\n","          4305,   390,  7126,  8925, 28747,   851,  6251,   349,  1132, 28723,\n","           560,   264, 15110,   607, 12501,  2229, 28725,   272,  7126,  8925,\n","           297,   624,  8777,   541,   347,  1275,  9646,   395,   272,  2118,\n","          3471,   297,   272,  1348,  8777, 28725,  5374,   298,  4240,  1293,\n","         16438,   302,   272, 19955, 28723,   851, 17301,   349,  2651,   390,\n","           272, 13378, 15921,  2118,   442,   272, 13378,  7559, 28733,   262,\n","         28733, 24227, 16117, 28723,    13,    13,  5816,   994, 28725,   272,\n","          4714,  4372,   349,   325, 28715, 28731,   325, 28710,   557,   325,\n","          2773,   557,   304,   325, 19747, 28731,   865, 28723,     2]],\n","       device='cuda:0')\n","output_token_ids: tensor([  415,  4714,  6251,   349,   325, 28715,  1329,   325, 28710,   557,\n","          325,  2773,   557,   304,   325, 19747, 28731,   865, 28723,    13,\n","           13, 15423,   349,   272, 13268,   354,  1430,  6251, 28747,    13,\n","           13, 28710, 28731,   415,  5287,   541,   347,  3716,   346, 11909,\n","         1413,  7681,  8429,   302,   451,  6687,   298,  1430,  8777, 28747,\n","          851,  6251,   349,   459,  8134,  1132, 28723,  4023,   378,   349,\n","         1132,   369,   451,  6687,   541,   347,  1307,   298, 11679,   272,\n","         5287,   354,  1430,  8777, 22264, 28725,   456,  4431,  1235,   459,\n","         2708,   354,   272,  4628, 17301,  1444,   272,  7126,  8925,   304,\n","          272,  2118,  3471,   297,   799, 12501, 28723,   851, 17301,   541,\n","         1736,   298,  4240,  1293, 16438, 28725,   579,   378,   349,  6741,\n","          459, 11572,   298, 11679,   272,  5287,   302,   264, 15110,   607,\n","        12501,  2229,   297,   456,  1069, 28723,    13,    13,  2773, 28731,\n","          415,  7126,  8925,   993,   347,  1275,  9646,   395,   272,  2118,\n","         3471,   297,   799, 12501, 28747,   851,  6251,   349,  1132, 28723,\n","          560,   264, 15110,   607, 12501,  2229, 28725,   272,  7126,  8925,\n","          297,   624,  8777,   541,   347,  1275,  9646,   395,   272,  2118,\n","         3471,   297,   799, 12501, 28725,  5374,   298,  4240,  1293, 16438,\n","          302,   272, 19955, 28723,   851, 17301,   349,  2651,   390,   272,\n","        15110,   607, 12501, 16117, 28723,    13,    13, 19747, 28731,  1094,\n","         4993,   302, 28705, 28750,  6255, 28735,   682,  1736,   298,   521,\n","         6309,  1293,   562,   297, 28627,  5621, 16438, 28747,   851,  6251,\n","          349,   459,  8134,  1132, 28723,  4023, 28705, 28750,  6255, 28735,\n","          541,   347,  1307,   298, 20545,   272, 15110,   607, 12501, 16117,\n","        28725,   378,  1235,   459, 11867,  3084,  9096, 16438,   302,   272,\n","        19955, 28723,   415, 12832,   302,   272, 16438,  9618,   356,   272,\n","         3716,   472,   304,  6342,   302,   272, 28166,  8925,  1307,   297,\n","          272, 28705, 28750,  6255, 28735, 10722, 28723,    13,    13,   449,\n","        28731,   415,  7126,  8925,   993,   347,  1275,  9646,   395,   272,\n","         2118,  3471,   297,   272, 12501,   297,   690,   590,  4305,   390,\n","         7126,  8925, 28747,   851,  6251,   349,  1132, 28723,   560,   264,\n","        15110,   607, 12501,  2229, 28725,   272,  7126,  8925,   297,   624,\n","         8777,   541,   347,  1275,  9646,   395,   272,  2118,  3471,   297,\n","          272,  1348,  8777, 28725,  5374,   298,  4240,  1293, 16438,   302,\n","          272, 19955, 28723,   851, 17301,   349,  2651,   390,   272, 13378,\n","        15921,  2118,   442,   272, 13378,  7559, 28733,   262, 28733, 24227,\n","        16117, 28723,    13,    13,  5816,   994, 28725,   272,  4714,  4372,\n","          349,   325, 28715, 28731,   325, 28710,   557,   325,  2773,   557,\n","          304,   325, 19747, 28731,   865, 28723,     2], device='cuda:0')\n","output_tokens: ['▁The', '▁correct', '▁statement', '▁is', '▁(', 'd', '):', '▁(', 'i', '),', '▁(', 'ii', '),', '▁and', '▁(', 'iii', ')', '▁only', '.', '<0x0A>', '<0x0A>', 'Here', '▁is', '▁the', '▁explanation', '▁for', '▁each', '▁statement', ':', '<0x0A>', '<0x0A>', 'i', ')', '▁The', '▁parameters', '▁can', '▁be', '▁valid', 'ly', '▁estimated', '▁using', '▁separate', '▁applications', '▁of', '▁O', 'LS', '▁to', '▁each', '▁equation', ':', '▁This', '▁statement', '▁is', '▁not', '▁entirely', '▁true', '.', '▁While', '▁it', '▁is', '▁true', '▁that', '▁O', 'LS', '▁can', '▁be', '▁used', '▁to', '▁estimate', '▁the', '▁parameters', '▁for', '▁each', '▁equation', '▁separately', ',', '▁this', '▁approach', '▁does', '▁not', '▁account', '▁for', '▁the', '▁potential', '▁correlation', '▁between', '▁the', '▁independent', '▁variables', '▁and', '▁the', '▁error', '▁terms', '▁in', '▁other', '▁equations', '.', '▁This', '▁correlation', '▁can', '▁lead', '▁to', '▁bi', 'ased', '▁estimates', ',', '▁so', '▁it', '▁is', '▁generally', '▁not', '▁recommended', '▁to', '▁estimate', '▁the', '▁parameters', '▁of', '▁a', '▁simultane', 'ous', '▁equations', '▁model', '▁in', '▁this', '▁way', '.', '<0x0A>', '<0x0A>', 'ii', ')', '▁The', '▁independent', '▁variables', '▁may', '▁be', '▁cor', 'related', '▁with', '▁the', '▁error', '▁terms', '▁in', '▁other', '▁equations', ':', '▁This', '▁statement', '▁is', '▁true', '.', '▁In', '▁a', '▁simultane', 'ous', '▁equations', '▁model', ',', '▁the', '▁independent', '▁variables', '▁in', '▁one', '▁equation', '▁can', '▁be', '▁cor', 'related', '▁with', '▁the', '▁error', '▁terms', '▁in', '▁other', '▁equations', ',', '▁leading', '▁to', '▁bi', 'ased', '▁estimates', '▁of', '▁the', '▁coefficients', '.', '▁This', '▁correlation', '▁is', '▁known', '▁as', '▁the', '▁simultane', 'ous', '▁equations', '▁bias', '.', '<0x0A>', '<0x0A>', 'iii', ')', '▁An', '▁application', '▁of', '▁', '2', 'SL', 'S', '▁would', '▁lead', '▁to', '▁un', 'bi', 'ased', '▁but', '▁in', 'efficient', '▁parameter', '▁estimates', ':', '▁This', '▁statement', '▁is', '▁not', '▁entirely', '▁true', '.', '▁While', '▁', '2', 'SL', 'S', '▁can', '▁be', '▁used', '▁to', '▁eliminate', '▁the', '▁simultane', 'ous', '▁equations', '▁bias', ',', '▁it', '▁does', '▁not', '▁necessarily', '▁provide', '▁efficient', '▁estimates', '▁of', '▁the', '▁coefficients', '.', '▁The', '▁efficiency', '▁of', '▁the', '▁estimates', '▁depends', '▁on', '▁the', '▁valid', 'ity', '▁and', '▁strength', '▁of', '▁the', '▁instrumental', '▁variables', '▁used', '▁in', '▁the', '▁', '2', 'SL', 'S', '▁procedure', '.', '<0x0A>', '<0x0A>', 'iv', ')', '▁The', '▁independent', '▁variables', '▁may', '▁be', '▁cor', 'related', '▁with', '▁the', '▁error', '▁terms', '▁in', '▁the', '▁equations', '▁in', '▁which', '▁they', '▁appear', '▁as', '▁independent', '▁variables', ':', '▁This', '▁statement', '▁is', '▁true', '.', '▁In', '▁a', '▁simultane', 'ous', '▁equations', '▁model', ',', '▁the', '▁independent', '▁variables', '▁in', '▁one', '▁equation', '▁can', '▁be', '▁cor', 'related', '▁with', '▁the', '▁error', '▁terms', '▁in', '▁the', '▁same', '▁equation', ',', '▁leading', '▁to', '▁bi', 'ased', '▁estimates', '▁of', '▁the', '▁coefficients', '.', '▁This', '▁correlation', '▁is', '▁known', '▁as', '▁the', '▁classical', '▁measurement', '▁error', '▁or', '▁the', '▁classical', '▁errors', '-', 'in', '-', 'variables', '▁bias', '.', '<0x0A>', '<0x0A>', 'There', 'fore', ',', '▁the', '▁correct', '▁answer', '▁is', '▁(', 'd', ')', '▁(', 'i', '),', '▁(', 'ii', '),', '▁and', '▁(', 'iii', ')', '▁only', '.']\n","User: "]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-5dba8b20b491>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["import os\n","\n","# Specify the file path\n","file_path = \"/content/drive/MyDrive/11868/mixtral-offloading/expert_cache_log.txt\"\n","\n","# Check if the file exists before removing\n","if os.path.exists(file_path):\n","    os.remove(file_path)\n","    print(f\"File {file_path} has been removed.\")\n","else:\n","    print(f\"File {file_path} does not exist.\")\n","\n","# Specify the file path\n","file_path = \"/content/drive/MyDrive/11868/mixtral-offloading/expert_cache.tsv\"\n","\n","# Check if the file exists before removing\n","if os.path.exists(file_path):\n","    os.remove(file_path)\n","    print(f\"File {file_path} has been removed.\")\n","else:\n","    print(f\"File {file_path} does not exist.\")\n","\n","file_path = \"/content/drive/MyDrive/11868/mixtral-offloading/custom_layer_log.txt\"\n","\n","# Check if the file exists before removing\n","if os.path.exists(file_path):\n","    os.remove(file_path)\n","    print(f\"File {file_path} has been removed.\")\n","else:\n","    print(f\"File {file_path} does not exist.\")\n","\n","file_path = \"/content/drive/MyDrive/11868/mixtral-offloading/generated_tokens.txt\"\n","\n","# Check if the file exists before removing\n","if os.path.exists(file_path):\n","    os.remove(file_path)\n","    print(f\"File {file_path} has been removed.\")\n","else:\n","    print(f\"File {file_path} does not exist.\")"],"metadata":{"id":"lIN-2UujZj5-","executionInfo":{"status":"ok","timestamp":1712706963501,"user_tz":240,"elapsed":182,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9f05704b-995a-489e-da0c-ae5b1f411646"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["File /content/drive/MyDrive/11868/mixtral-offloading/expert_cache_log.txt does not exist.\n","File /content/drive/MyDrive/11868/mixtral-offloading/expert_cache.tsv has been removed.\n","File /content/drive/MyDrive/11868/mixtral-offloading/custom_layer_log.txt has been removed.\n","File /content/drive/MyDrive/11868/mixtral-offloading/generated_tokens.txt has been removed.\n"]}]},{"cell_type":"code","source":["# output = \"Hello! I am a helpful and respectful AI assistant, designed to accurately answer questions, provide suggestions and give a broad range of information across various topics. I strive to ensure user satisfaction while abiding by ethical guidelines and privacy protocols.\"\n","#\n"],"metadata":{"id":"9q6BnMuausi6","executionInfo":{"status":"aborted","timestamp":1712705326651,"user_tz":240,"elapsed":4,"user":{"displayName":"Shuning Lin","userId":"05509187697367389947"}}},"execution_count":null,"outputs":[]}]}